import numpy as np
import streamlit as st
import base64
import pytube
import os
import subprocess 
import librosa
import tempfile 
from pydub import AudioSegmentimport os
import sqlite3
import sqlite3
import bcrypt
import re  # Th√™m th∆∞ vi·ªán ki·ªÉm tra email h·ª£p l·ªá
from openai import OpenAI
import numpy as np
import streamlit as st
import base64
import pytube
import os
import subprocess 
import librosa
import tempfile 
from pydub import AudioSegment
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib.colors import Normalize
import tensorflow as tf
from statistics import mode
from tensorflow import keras
from keras import regularizers
from keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Activation)
from streamlit_option_menu import option_menu
import time
from dotenv import load_dotenv
from supabase import create_client, Client

st.set_page_config(page_title="Music AI Website", layout="wide")
# Load API key t·ª´ file .env
load_dotenv()
#openai.api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)

def generate_lyrics(prompt):
    """G·ª≠i prompt ƒë·∫øn OpenAI API ƒë·ªÉ t·∫°o l·ªùi b√†i h√°t"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o",  # Ho·∫∑c "gpt-3.5-turbo" n·∫øu t√†i kho·∫£n kh√¥ng c√≥ quy·ªÅn truy c·∫≠p GPT-4
            messages=[
                {"role": "system", "content": "B·∫°n l√† m·ªôt nh·∫°c sƒ© s√°ng t√°c l·ªùi b√†i h√°t chuy√™n nghi·ªáp."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.9,
            max_tokens=900
        )

        # ‚úÖ L·∫•y n·ªôi dung ph·∫£n h·ªìi ƒë√∫ng c√°ch
        return response.choices[0].message.content  

    except Exception as e:
        return f"‚ö†Ô∏è L·ªói khi t·∫°o l·ªùi b√†i h√°t: {str(e)}"

# Test th·ª≠ h√†m
#prompt = "Vi·∫øt l·ªùi b√†i h√°t v·ªÅ t√¨nh y√™u m√πa thu"
#lyrics = generate_lyrics(prompt)
#print(lyrics)

st.markdown(
    """
    <style>
        /* ƒê·∫∑t h√¨nh n·ªÅn chung cho to√†n b·ªô trang */
        body, .stApp {
            background: url("https://i.pinimg.com/originals/c3/aa/cd/c3aacdb10d1c0d550b7fa08b6d0bddb1.jpg") no-repeat center center fixed;
            background-size: cover;
        }

        /* Sidebar trong su·ªët, gi·ªØ n·ªÅn ƒë·ªìng nh·∫•t */
        [data-testid="stSidebar"] {
            background: rgba(255, 255, 255, 0.1) !important;
            backdrop-filter: blur(5px);
            border-right: 1px solid rgba(255, 255, 255, 0.2);
        }

        /* L√†m m·ªù nh·∫π ph·∫ßn n·ªôi dung ch√≠nh ƒë·ªÉ n·ªïi b·∫≠t h∆°n */
        .stApp > div:nth-child(1) {
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 10px;
        }

        /* Ch·ªânh m√†u ch·ªØ ƒë·ªÉ d·ªÖ ƒë·ªçc tr√™n n·ªÅn */
        h1, h2, h3, p {
            color: white !important;
        }

        /* T√πy ch·ªânh n√∫t b·∫•m */
        .stButton>button {
            background: linear-gradient(to right, #ff758c, #ff7eb3);
            color: white;
            font-size: 16px;
            border: none;
            padding: 10px;
            border-radius: 8px;
            transition: 0.3s;
        }

        .stButton>button:hover {
            transform: scale(1.05);
            background: linear-gradient(to right, #ff5f6d, #ffc371);
        }

        /* √î nh·∫≠p li·ªáu trong su·ªët */
        .stTextInput>div>div>input {
            background-color: rgba(255, 255, 255, 0.2) !important;
            border-radius: 5px;
            border: 1px solid rgba(255, 255, 255, 0.5) !important;
            padding: 10px !important;
            font-size: 14px !important;
            color: white !important;
        }

    </style>
    """,
    unsafe_allow_html=True
)





# T·∫°o menu Sidebar c√≥ icon
with st.sidebar:
    st.image("https://media.giphy.com/media/xThtapIXXGuYEnqNgU/giphy.gif", use_container_width=True)

    menu = option_menu(
        menu_title="Navigation",
        options=["Home", "Create Lyrics", "Feel The Beat", "Classify", "Explore", "Library", "Search"],
        icons=["house", "music-note-list", "soundwave", "graph-up", "globe", "book", "search"],
        menu_icon="menu-button-wide",
        default_index=0,
        styles={
            "container": {"background-color": "rgba(0,0,0,0.8)", "padding": "5px"},
            "icon": {"color": "#feb47b", "font-size": "20px"},
            "nav-link": {"font-size": "18px", "color": "#ffffff", "text-align": "left", "margin": "5px"},
            "nav-link-selected": {"background-color": "#ff7e5f"},
        }
    )




# N·∫øu ch·ªçn "Classify", hi·ªÉn th·ªã n·ªôi dung n√†y
if menu == "Classify":
    st.markdown("<h1 style='text-align: center; color: white;'>Music Genre Recognition</h1>", unsafe_allow_html=True)

    # Upload file mp3
    st.write("## Upload an MP3 file to classify:")
    mp3_file = st.file_uploader("Upload an audio file", type=["mp3"], label_visibility="collapsed")    
    
    if mp3_file is not None:
        st.write("**Play the song below:**")
        st.audio(mp3_file, "audio/mp3")

        # H√†m chuy·ªÉn ƒë·ªïi MP3 sang WAV
        def convert_mp3_to_wav(music_file):  
            sound = AudioSegment.from_mp3(music_file)
            sound.export("music_file.wav", format="wav")

        # H√†m t·∫°o Mel Spectrogram
        def create_melspectrogram(wav_file):  
            y, sr = librosa.load(wav_file)  
            mel_spec = librosa.power_to_db(librosa.feature.melspectrogram(y=y, sr=sr))    
            plt.figure(figsize=(10, 5))
            plt.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])
            librosa.display.specshow(mel_spec, x_axis="time", y_axis='mel', sr=sr)
            plt.margins(0)
            plt.savefig('melspectrogram.png')

        # X√¢y d·ª±ng m√¥ h√¨nh CNN
        def GenreModel(input_shape=(100,200,4), classes=10):
            classifier = Sequential()
            classifier.add(Conv2D(8, (3, 3), input_shape=input_shape, activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Conv2D(16, (3, 3), activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Conv2D(32, (3, 3), activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Conv2D(64, (3, 3), activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Conv2D(128, (3, 3), activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Flatten())
            classifier.add(Dropout(0.5))
            classifier.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))
            classifier.add(Dropout(0.25))
            classifier.add(Dense(10, activation='softmax'))
            classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
            return classifier

        # D·ª± ƒëo√°n th·ªÉ lo·∫°i nh·∫°c
        def predict(image_data, model):   
            image = img_to_array(image_data)   
            image = np.reshape(image, (1, 100, 200, 4))   
            prediction = model.predict(image / 255)   
            prediction = prediction.reshape((10,))     
            class_label = np.argmax(prediction)     
            return class_label, prediction

        # Nh√£n c·ªßa c√°c th·ªÉ lo·∫°i nh·∫°c
        class_labels = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']

        # Load m√¥ h√¨nh
        model = GenreModel(input_shape=(100, 200, 4), classes=10)
        model.load_weights("music_genre_recog_model.h5")

        # Hi·ªáu ·ª©ng loading
        with st.spinner("üîç Analyzing music genre..."):
            time.sleep(2)

        # Chuy·ªÉn ƒë·ªïi file v√† t·∫°o spectrogram
        convert_mp3_to_wav(mp3_file)
        audio_full = AudioSegment.from_wav('music_file.wav')

        class_labels_total = []
        predictions_total = []
        for w in range(int(round(len(audio_full) / 3000, 0))):
            audio_3sec = audio_full[3 * (w) * 1000: 3 * (w + 1) * 1000]
            audio_3sec.export(out_f="audio_3sec.wav", format="wav")
            create_melspectrogram("audio_3sec.wav")
            image_data = load_img('melspectrogram.png', color_mode='rgba', target_size=(100, 200))   
            class_label, prediction = predict(image_data, model)
            class_labels_total.append(class_label)
            predictions_total.append(prediction)

        # L·∫•y th·ªÉ lo·∫°i c√≥ d·ª± ƒëo√°n cao nh·∫•t
        class_label_final = mode(class_labels_total)
        predictions_final = np.mean(predictions_total, axis=0)

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        st.success(f"‚úÖ The genre of your song is: **{class_labels[class_label_final]}**")
        # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì x√°c su·∫•t d·ª± ƒëo√°n
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.bar(class_labels, predictions_final, color=cm.viridis(np.linspace(0, 1, len(class_labels))))
        ax.set_xlabel("Music Genre")
        ax.set_ylabel("Prediction Probability")
        ax.set_title("Genre Prediction Probability Distribution")
        ax.set_xticklabels(class_labels, rotation=45)
        # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì trong Streamlit
        st.pyplot(fig)



import requests
# =================== GIAO DI·ªÜN CHO CREATE LYRICS ===================
if menu == "Create Lyrics":
    st.markdown("<h1>üé∂ AI Lyric Generator üéµ</h1>", unsafe_allow_html=True)

    # Ng∆∞·ªùi d√πng nh·∫≠p th·ªÉ lo·∫°i nh·∫°c v√† ch·ªß ƒë·ªÅ
    genre = st.selectbox("üéº Ch·ªçn th·ªÉ lo·∫°i nh·∫°c:", ["Pop", "Rock", "Hip-Hop", "Jazz", "Ballad", "EDM"])
    theme = st.text_input("‚úçÔ∏è Nh·∫≠p ch·ªß ƒë·ªÅ b√†i h√°t (VD: T√¨nh y√™u, M√πa thu, Tu·ªïi tr·∫ª, ...)")
    mood = st.radio("üé≠ Ch·ªçn c·∫£m x√∫c:", ["Vui v·∫ª", "Bu·ªìn", "H√†o h·ª©ng", "Th∆∞ gi√£n", "K·ªãch t√≠nh"])

    if st.button("üé§ S√°ng t√°c ngay!"):
        if theme.strip():
            with st.spinner("üé∂ AI ƒëang s√°ng t√°c l·ªùi b√†i h√°t cho b·∫°n..."):
                prompt = f"H√£y vi·∫øt l·ªùi b√†i h√°t th·ªÉ lo·∫°i {genre} v·ªÅ ch·ªß ƒë·ªÅ '{theme}', v·ªõi c·∫£m x√∫c {mood}."
                lyrics = generate_lyrics(prompt)
                print(lyrics)
                st.text_area("üéº L·ªùi b√†i h√°t AI t·∫°o:", lyrics, height=300)
        else:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p ch·ªß ƒë·ªÅ b√†i h√°t tr∆∞·ªõc khi t·∫°o!")
       

if menu == "Feel The Beat":
    st.title("üéµ Feel The Beat - T·∫°o Nh·∫°c AI")

    # Nh·∫≠p API Token
    api_token = st.text_input("üîë Nh·∫≠p API Token:", type="password")

    # Nh·∫≠p m√¥ t·∫£ nh·∫°c c·∫ßn t·∫°o
    prompt = st.text_area("üí° Nh·∫≠p m√¥ t·∫£ b·∫£n nh·∫°c b·∫°n mu·ªën t·∫°o:", 
                          placeholder="M·ªôt b·∫£n nh·∫°c piano th∆∞ gi√£n v·ªõi giai ƒëi·ªáu nh·∫π nh√†ng...")

    # C√°c t√πy ch·ªçn nh·∫°c
    style = st.selectbox("üéº Ch·ªçn phong c√°ch nh·∫°c:", ["Classical", "Jazz", "Lo-fi", "Ambient", "Rock"])
    title = st.text_input("üé∂ ƒê·∫∑t t√™n b·∫£n nh·∫°c:", "My AI Music")
    instrumental = st.checkbox("üéª Nh·∫°c kh√¥ng l·ªùi?", value=False)

    # X·ª≠ l√Ω khi b·∫•m n√∫t
    if st.button("üéß Feel The Beat"):
        if not api_token or not prompt:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p API Token v√† m√¥ t·∫£ nh·∫°c!")
        else:
            # G·ª≠i y√™u c·∫ßu API t·∫°o nh·∫°c
            api_url = "https://apibox.erweima.ai/api/v1/generate"
            headers = {"Authorization": f"Bearer {api_token}", "Content-Type": "application/json"}
            data = {
                "prompt": prompt,
                "style": style,
                "title": title,
                "customMode": True,
                "instrumental": instrumental,
                "model": "V3_5",
                "callBackUrl": "https://api.example.com/callback"
            }

            with st.spinner("üéº ƒêang t·∫°o nh·∫°c..."):
                response = requests.post(api_url, json=data, headers=headers)

            # X·ª≠ l√Ω k·∫øt qu·∫£
            if response.status_code == 200:
                task_id = response.json().get("data", {}).get("taskId", None)
                st.write("üìå Task ID:", task_id)  # Debug Task ID

                if not task_id:
                    st.error("üö® API kh√¥ng tr·∫£ v·ªÅ Task ID!")
                else:
                    check_url = f"https://apibox.erweima.ai/api/v1/generate/record-info?taskId={task_id}"
                    headers = {
                        "Authorization": f"Bearer {api_token}",
                        "Accept": "application/json"
                    }

                    st.info("‚è≥ ƒêang ch·ªù nh·∫°c... (t·ªëi ƒëa 5 ph√∫t)")
                    audio_url = None

                    for _ in range(60):  # L·∫∑p t·ªëi ƒëa 60 l·∫ßn (5 ph√∫t)
                        check_response = requests.get(check_url, headers=headers)

                        if check_response.status_code == 200:
                            try:
                                music_info = check_response.json()
                                data = music_info.get("data", {})
                                status = data.get("status", "PENDING")  # Ki·ªÉm tra tr·∫°ng th√°i

                                if status == "SUCCESS":
                                    response_data = data.get("response", {})
                                    suno_data = response_data.get("sunoData", [])

                                    if suno_data and isinstance(suno_data, list):
                                        audio_url = suno_data[0].get("audioUrl")

                                if audio_url:
                                    break  # D·ª´ng v√≤ng l·∫∑p n·∫øu ƒë√£ c√≥ nh·∫°c

                            except Exception as e:
                                st.error(f"üö® L·ªói khi x·ª≠ l√Ω JSON t·ª´ API: {e}")
                                st.write("üìÑ N·ªôi dung API tr·∫£ v·ªÅ:", check_response.text)
                                break  # N·∫øu l·ªói, d·ª´ng lu√¥n
                        time.sleep(5)  # Ch·ªù 5 gi√¢y tr∆∞·ªõc khi ki·ªÉm tra l·∫°i

                    # Ki·ªÉm tra k·∫øt qu·∫£ sau v√≤ng l·∫∑p
                    if audio_url:
                        st.success(f"üéµ Nh·∫°c ƒë√£ s·∫µn s√†ng: [{title}]({audio_url})")
                        st.audio(audio_url, format="audio/mp3")
                    else:
                        st.warning("‚è≥ Nh·∫°c ch∆∞a s·∫µn s√†ng sau 5 ph√∫t, h√£y th·ª≠ l·∫°i sau!")
            else:
                st.error(f"üö® L·ªói API: {response.json().get('error', 'Kh√¥ng r√µ l·ªói!')}")
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib.colors import Normalize
import tensorflow as tf
from statistics import mode
from tensorflow import keras
from keras import regularizers
from keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Activation)
from streamlit_option_menu import option_menu
import time
from openai import OpenAI  
import openai  

# C·∫•u h√¨nh trang
st.set_page_config(page_title="Music AI Website", layout="wide")

# T√πy ch·ªânh CSS cho Sidebar
st.markdown(
    """
    <style>
        [data-testid="stSidebar"] {
            background-image: url("https://cdn.pixabay.com/photo/2024/02/26/14/13/sky-8598072_1280.jpg");
            background-size: cover;
        }
        .css-1d391kg {
            background-color: rgba(0,0,0,0.8) !important;
        }
        .stButton>button {
            width: 100%;
            border-radius: 10px;
            background: linear-gradient(to right, #ff7e5f, #feb47b);
            color: white;
        }
        .stButton>button:hover {
            transform: scale(1.05);
            transition: 0.3s;
        }
    </style>
    """,
    unsafe_allow_html=True
)

# T·∫°o menu Sidebar c√≥ icon
with st.sidebar:
    st.markdown(
        '<img src="https://media.giphy.com/media/xThtapIXXGuYEnqNgU/giphy.gif" width="100%">',
        unsafe_allow_html=True
    )
    menu = option_menu(
        menu_title="Navigation",
        options=["Home", "Create Lyric", "Feel The Beat", "Classify", "Explore", "Library", "Search"],
        icons=["house", "music-note-list", "soundwave", "graph-up", "globe", "book", "search"],
        menu_icon="menu-button-wide",
        default_index=0,
        styles={
            "container": {"background-color": "rgba(0,0,0,0.8)", "padding": "5px"},
            "icon": {"color": "#feb47b", "font-size": "20px"},
            "nav-link": {"font-size": "18px", "color": "#ffffff", "text-align": "left", "margin": "5px"},
            "nav-link-selected": {"background-color": "#ff7e5f"},
        }
    )
    




# --- T·∫°o L·ªùi B√†i H√°t B·∫±ng AI ---
if menu == "Create Lyric":
    st.title("üéº T·∫°o L·ªùi B√†i H√°t B·∫±ng AI")

    # Nh·∫≠p API Key
    api_key = st.text_input("üîë Nh·∫≠p API Key c·ªßa b·∫°n:", type="password")

    # Nh·∫≠p √Ω t∆∞·ªüng b√†i h√°t
    song_idea = st.text_area("üí° Nh·∫≠p √Ω t∆∞·ªüng cho b√†i h√°t:", placeholder="Vi·∫øt v·ªÅ t√¨nh y√™u, m√πa thu, ho·∫∑c b·∫•t k·ª≥ ƒëi·ªÅu g√¨ b·∫°n mu·ªën...")

    # X·ª≠ l√Ω khi ng∆∞·ªùi d√πng nh·∫•n n√∫t t·∫°o l·ªùi b√†i h√°t
    if st.button("‚ú® T·∫°o L·ªùi B√†i H√°t"):
        if not api_key:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p API Key!")
        elif not song_idea:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p √Ω t∆∞·ªüng b√†i h√°t!")
        else:
            try:
                # G·ª≠i y√™u c·∫ßu ƒë·∫øn OpenAI GPT
                openai.api_key = api_key  # Truy·ªÅn API Key ƒë√∫ng c√°ch
                
                response = openai.ChatCompletion.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": f"Vi·∫øt l·ªùi b√†i h√°t theo phong c√°ch chuy√™n nghi·ªáp d·ª±a tr√™n √Ω t∆∞·ªüng: {song_idea}"}],
                    max_tokens=300
                )

                # L·∫•y n·ªôi dung tr·∫£ v·ªÅ t·ª´ API
                lyrics = response["choices"][0]["message"]["content"].strip()

                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                st.subheader("üé∂ L·ªùi B√†i H√°t C·ªßa B·∫°n:")
                st.text_area("üìú", lyrics, height=300)

            except openai.error.OpenAIError as e:
                st.error(f"üö® L·ªói t·ª´ OpenAI: {e}")
            except Exception as e:
                st.error(f"üö® L·ªói h·ªá th·ªëng: {e}")

# N·∫øu ch·ªçn "Classify", hi·ªÉn th·ªã n·ªôi dung n√†y
if menu == "Classify":
    st.markdown("<h1 style='text-align: center; color: white;'>Music Genre Recognition</h1>", unsafe_allow_html=True)

    # Upload file mp3
    st.write("## Upload an MP3 file to classify:")
    mp3_file = st.file_uploader("Upload an audio file", type=["mp3"], label_visibility="collapsed")    
    
    if mp3_file is not None:
        st.write("**Play the song below:**")
        st.audio(mp3_file, "audio/mp3")

        # H√†m chuy·ªÉn ƒë·ªïi MP3 sang WAV
        def convert_mp3_to_wav(music_file):  
            sound = AudioSegment.from_mp3(music_file)
            sound.export("music_file.wav", format="wav")

        # H√†m t·∫°o Mel Spectrogram
        def create_melspectrogram(wav_file):  
            y, sr = librosa.load(wav_file)  
            mel_spec = librosa.power_to_db(librosa.feature.melspectrogram(y=y, sr=sr))    
            plt.figure(figsize=(10, 5))
            plt.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])
            librosa.display.specshow(mel_spec, x_axis="time", y_axis='mel', sr=sr)
            plt.margins(0)
            plt.savefig('melspectrogram.png')

        # X√¢y d·ª±ng m√¥ h√¨nh CNN
        def GenreModel(input_shape=(100,200,4), classes=10):
            classifier = Sequential()
            classifier.add(Conv2D(8, (3, 3), input_shape=input_shape, activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Conv2D(16, (3, 3), activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Conv2D(32, (3, 3), activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Conv2D(64, (3, 3), activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Conv2D(128, (3, 3), activation='relu'))
            classifier.add(MaxPooling2D(pool_size=(2, 2)))
            classifier.add(Flatten())
            classifier.add(Dropout(0.5))
            classifier.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))
            classifier.add(Dropout(0.25))
            classifier.add(Dense(10, activation='softmax'))
            classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
            return classifier

        # D·ª± ƒëo√°n th·ªÉ lo·∫°i nh·∫°c
        def predict(image_data, model):   
            image = img_to_array(image_data)   
            image = np.reshape(image, (1, 100, 200, 4))   
            prediction = model.predict(image / 255)   
            prediction = prediction.reshape((10,))     
            class_label = np.argmax(prediction)     
            return class_label, prediction

        # Nh√£n c·ªßa c√°c th·ªÉ lo·∫°i nh·∫°c
        class_labels = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']

        # Load m√¥ h√¨nh
        model = GenreModel(input_shape=(100, 200, 4), classes=10)
        model.load_weights("music_genre_recog_model.h5")

        # Hi·ªáu ·ª©ng loading
        with st.spinner("üîç Analyzing music genre..."):
            time.sleep(2)

        # Chuy·ªÉn ƒë·ªïi file v√† t·∫°o spectrogram
        convert_mp3_to_wav(mp3_file)
        audio_full = AudioSegment.from_wav('music_file.wav')

        class_labels_total = []
        predictions_total = []
        for w in range(int(round(len(audio_full) / 3000, 0))):
            audio_3sec = audio_full[3 * (w) * 1000: 3 * (w + 1) * 1000]
            audio_3sec.export(out_f="audio_3sec.wav", format="wav")
            create_melspectrogram("audio_3sec.wav")
            image_data = load_img('melspectrogram.png', color_mode='rgba', target_size=(100, 200))   
            class_label, prediction = predict(image_data, model)
            class_labels_total.append(class_label)
            predictions_total.append(prediction)

        # L·∫•y th·ªÉ lo·∫°i c√≥ d·ª± ƒëo√°n cao nh·∫•t
        class_label_final = mode(class_labels_total)
        predictions_final = np.mean(predictions_total, axis=0)

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        st.success(f"‚úÖ The genre of your song is: **{class_labels[class_label_final]}**")
        # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì x√°c su·∫•t d·ª± ƒëo√°n
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.bar(class_labels, predictions_final, color=cm.viridis(np.linspace(0, 1, len(class_labels))))
        ax.set_xlabel("Music Genre")
        ax.set_ylabel("Prediction Probability")
        ax.set_title("Genre Prediction Probability Distribution")
        ax.set_xticklabels(class_labels, rotation=45)
        # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì trong Streamlit
        st.pyplot(fig)

